---
title: "Machine Learning 1"
author: "Gabrielle Meza (A13747395)"
date: "10/22/2021"
output:
  pdf_document: default
  html_document: default
---
#Important take aways:

**kmeans(x,centers=?)**, **hclust()**

hclust doesnt impose on you data immediately, like kmeans() would. 

hclust(dist(x)) take the distance matrix of x

##reminder: to insert r code without using button i console, can do control+option+i


#Clustering methods
Kmeans clustering in R is done with the 'kmeans()' function
here we will makeup some data to test and learn with it 

cbinds combines a vector into a dataset. y=rev takes the forward vector, and maes the next column the reverse of it. so here we should get an inverse dataset  

```{r}
tmp <- c(rnorm(30,3), rnorm(30,-3))
data <-cbind(x=tmp, y=rev(tmp))
data
plot(data)
```

We are now going to run kmeans on this run 'kmeans()' set k to 2 nstart 20. the thing with kmeans is you have to tell it how many clusters you want. once you print it out, it will send out information. Cluster means, is the center of the two groups. Km puts multiple things into a list, can combine vectors and such into a list think of it as a container, that is why $ is able to pull out a component.

the Clustering vector is 

```{r}
km <-kmeans(data, centers = 2, nstart=20)
km
```

> Q.  How many points are in each cluster?

When you use the $, it is looking for a colum or row tittle with that name.

```{r}
km$size
```

>Q. what 'component' of your result oject details cluster assignment/ membership?

```{r}
km$cluster
```

>Q. what 'component' of your result oject details cluster center?

```{r}
km$centers
```

>Q. Plot x colored by the kmeans cluster assignment and add cluster centers as blue?

Different col number is different colors. but to color your different groups based on the clustering, you can do **col=km$cluster**.

If you want to plot by cluster use **points(km$centers)** and other componets too, pch turns into a square, cex, makes the square bigger


```{r}
plot(data, col=km$cluster)

```

For this next graph, you need the plot component to be there and then can do points 
```{r}
plot(data, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=2)
```

#Hierarchial Clustering

we will use `hclust()`function on the same data as before and see how this method works. **dist(data)** is just a way to get the distance between points in you graph

```{r}
hc <- hclust( dist(data))
hc
```

But hclust has a ploting method

```{r}
plot(hc)

```


Clustering, finds points that are close to each other, draws the distance between them, then moves onto the next point, and then keeps going. you can see in this plot that it the two main division is the first 1-30, and then 30-60 that are separated. it means that these are two groups separated.using this tree analysis.

to find our membership vector we need to "cut" the tree and for this we use the **cutree()** funtion and tell it the height to cut at. 

```{r}
cutree(hc, h=7)
```

we can also use the **cutree()** and state the number of clusters we want. h cuts it into height seperation, and k cuts it into groups. k=2 cuts into 2 groups

```{r}
cutree(hc, k=2)
```

```{r}
grps <- cutree(hc, k=2)
plot(data, col=grps)
```

#Prinicpal Component analysis (PCA) 
PCA project the features into the principal components. The motivation is to reduce the features dimensionality while only losing a small amount of information. 

The first principal (PC1) follows the "best fit" through the data points. These data have the maximum varience 

The second principal (PC2) follows the spread (kinda up and down) of the points.

these two components form new axis for the data that can better fit our data and view multidemensional data. can also find outlines better 

#Now time for more work of this stuff- multidemensional 

PCA is a super useful ananlysis method when you have lots of dimensions in your data...

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
```

We shall say that the 17 food types are the variables and the 4 countries are the observations. This would be equivalent to our samples and genes respectively from the lecture example (and indeed the second main example further below).

How many rows and cols? can use **dim(x)** for the dimensions, and just x to get to get the componets..?

```{r}
dim(x)
```
```{r}
x
```
Here^^ you can see that it is not in the correct format, there should be . So we need to argure with R to fix.

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

```{r}
rownames(x) <-x[,1]
x <- x[,-1]
x
```

This works, but if you run it again it will keep taking off a column everytime you run it. So to fix this you incorporate in the way you read. add **row.names=1**

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
dim(x)
head(x)
```
rainbow is a function that pulls the colors of the rainbows. if you did rainbow(10) it would give you the color codes for the first 10. in this example, we are using a diff color of the rainbow for each food type. 
Now we can start and try to visulaize this data:

```{r}
barplot(as.matrix(x), beside= TRUE, col = rainbow(nrow(x)))
```
> Q3: Changing what optional argument in the above barplot() function results in the following plot?

Changing besides to FALSE. meaning not beside each other

```{r}
barplot(as.matrix(x), beside= FALSE, col = rainbow(nrow(x)))
```
> Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

If it lies on the diagonal, that means it is the same between countries the amount of the category consumed. 

```{r}
mycols <- rainbow(17)
pairs(x,col= mycols, pch= 16)
```

>Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

there are more outliers in N. ireland. 

##PCA to the rescue!
Here we will use the base R function for PCA, which is called `pcomp()`.
This function was writted backwards, so you need to transpose this to fit with your data.
Can use **t(x)** to transpose

```{r}
#precomp( x ) 
pca <- prcomp(t(x))
summary(pca)
```

```{r}
plot(pca)
```

We want to score plot (a.ka. PCA plot). Basically of PC1 vs PC2

```{r}
attributes(pca)
```

use this^ to see what the different attibutes are. we are after the pca$x component for this plot... 

```{r}
plot(pca$x[,1:2])
```

^ this is a point for each country. lets make this fancier:

> Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
plot(pca$x[,1:2])
text(pca$x[,1:2], labels = colnames(x))
```


We can also examine the PCA "loadings" which tell us how much the original variables contribute to each new PC... This is in the rotation component. Along PC1 we can go along in a negative direction. The negative ones means that one country has way more of one catergory than another 

```{r}
pca$rotation
barplot(pca$rotation[,1], las=2)
```

##One More PCA for today. doing RNA-SEQ
```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

> Q10: How many genes and samples are in this data set?

```{r}
ncol(rna.data)
colnames(rna.data)
```

Now try to plot data. with PCA you want to rescale it becuase there can be high variablity of expression. that is why you set a scale. becuase no expression vs high expression would affect it. 

```{r}
pca.rna <- prcomp(t(rna.data), scale = TRUE)
summary(pca.rna)

```

We can see from this results that PC1 is were all the action is (92.6% of it in fact!). This indicates that we have successfully reduced a 100 dimensional data set down to only one dimension that retains the main essential (or principal) features of the original data. PC1 captures 92.6% of the original variance with the first two PCs capturing 94.9%. This is quite amazing!

More plotting:

```{r}
plot(pca.rna$x[,1:2])
text(pca.rna$x[,1], pca.rna$x[,2], labels = colnames(rna.data))
```







